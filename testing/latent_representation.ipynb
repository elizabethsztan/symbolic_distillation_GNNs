{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring the latent representations: activations of the message functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from model import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "data = load_data('../simulations/datasets/r1_n=3_dim=2_nt=1000_dt=0.005')\n",
    "X, y = data\n",
    "\n",
    "#make train, test, val sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/25: 100%|██████████| 313/313 [00:02<00:00, 124.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.9895, val loss: 1.6207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2/25: 100%|██████████| 313/313 [00:01<00:00, 204.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.7332, val loss: 1.4217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3/25: 100%|██████████| 313/313 [00:01<00:00, 176.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.5988, val loss: 1.2005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4/25: 100%|██████████| 313/313 [00:02<00:00, 148.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.4662, val loss: 1.0629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5/25: 100%|██████████| 313/313 [00:02<00:00, 152.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.4453, val loss: 1.0125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 6/25: 100%|██████████| 313/313 [00:02<00:00, 131.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.3608, val loss: 0.8514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 7/25: 100%|██████████| 313/313 [00:03<00:00, 81.50it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.3230, val loss: 0.8385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 8/25: 100%|██████████| 313/313 [00:03<00:00, 100.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.3334, val loss: 0.7555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 9/25: 100%|██████████| 313/313 [00:02<00:00, 106.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.2873, val loss: 0.6789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 10/25: 100%|██████████| 313/313 [00:02<00:00, 125.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.2542, val loss: 0.7638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 11/25: 100%|██████████| 313/313 [00:02<00:00, 116.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.2610, val loss: 0.7272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 12/25: 100%|██████████| 313/313 [00:02<00:00, 127.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.2483, val loss: 0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 13/25: 100%|██████████| 313/313 [00:02<00:00, 143.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.2402, val loss: 0.8347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 14/25: 100%|██████████| 313/313 [00:02<00:00, 154.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.2399, val loss: 0.6345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 15/25: 100%|██████████| 313/313 [00:02<00:00, 134.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.2510, val loss: 0.8689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 16/25: 100%|██████████| 313/313 [00:02<00:00, 151.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.2456, val loss: 0.6652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 17/25: 100%|██████████| 313/313 [00:02<00:00, 139.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.2365, val loss: 0.5629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 18/25: 100%|██████████| 313/313 [00:02<00:00, 112.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.2021, val loss: 0.6641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 19/25: 100%|██████████| 313/313 [00:02<00:00, 140.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.2127, val loss: 0.5739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 20/25: 100%|██████████| 313/313 [00:02<00:00, 122.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.2016, val loss: 0.6459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 21/25: 100%|██████████| 313/313 [00:03<00:00, 85.85it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.2020, val loss: 0.9208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 22/25: 100%|██████████| 313/313 [00:03<00:00, 97.43it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.2099, val loss: 0.7015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 23/25: 100%|██████████| 313/313 [00:07<00:00, 43.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.2012, val loss: 0.6841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 24/25: 100%|██████████| 313/313 [00:02<00:00, 106.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.1931, val loss: 0.5807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 25/25: 100%|██████████| 313/313 [00:03<00:00, 78.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.2214, val loss: 0.5969\n"
     ]
    }
   ],
   "source": [
    "cutoff = 10000\n",
    "val_cutoff = 2500\n",
    "\n",
    "#train the model on a small amount of data\n",
    "model = train((X_train[:cutoff], y_train[:cutoff]), (X_val[:cutoff], y_val[:cutoff]), 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x7fe241937d00>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TensorDataset(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 100])\n",
      "torch.Size([1, 6, 112])\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(TensorDataset(X_test[:1]), batch_size=1, shuffle=False)\n",
    "edge_index = get_edge_index(X_test.shape[1])\n",
    "#print(edge_index)\n",
    "tmp = 0\n",
    "for (nodes,) in dataloader:\n",
    "    s1 = nodes[:, edge_index[0]]\n",
    "    #print(s1)\n",
    "    s2 = nodes[:, edge_index[1]]\n",
    "    edge_features = torch.cat((s1, s2), dim=-1)\n",
    "    messages = model.edge_model(edge_features)\n",
    "    print(messages.shape)\n",
    "    batch_messages = torch.cat((s1, s2, messages), dim=-1)\n",
    "    print(batch_messages.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the message features along with other information from the trained model and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_message_features(model, input_data, batch_size=32):\n",
    "    \"\"\"\n",
    "    Make pd.Dataframe of messages and particle information\n",
    "    \n",
    "    Args:\n",
    "        model (NBodyGNN): Trained model\n",
    "        input_data (torch.Tensor): Input data with shape [no_timesteps, no_nodes, node_features]\n",
    "        batch_size (int): Size of batches to process\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing node features and message features\n",
    "        numpy.ndarray: just the message features\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()  \n",
    "    edge_index = get_edge_index(input_data.shape[1])\n",
    "    \n",
    "    # Create dataloader\n",
    "    dataset = TensorDataset(input_data)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    all_message_info = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for (nodes,) in dataloader:\n",
    "            x_i = nodes[:, edge_index[0]]  # Source nodes\n",
    "            x_j = nodes[:, edge_index[1]]  # Target nodes\n",
    "            \n",
    "            x = torch.cat((x_i, x_j), dim=-1)\n",
    "            messages = model.edge_model(x) #put thru edge model\n",
    "            #messages shape is [batch_size, num_edges, 100]\n",
    "            \n",
    "            #combine node features with messages along final axis\n",
    "            message_info = torch.cat((x_i, x_j, messages), dim=-1)\n",
    "            \n",
    "            #reshape batch_messages to be 2D for pandas: [batch_size * num_edges, features]\n",
    "            #batch info and edge info doesnt matter. only related feature info and latent messages\n",
    "            message_info = message_info.reshape(-1, message_info.shape[-1])\n",
    "            all_message_info.append(message_info)\n",
    "    \n",
    "    # Combine all batches\n",
    "    message_info = torch.cat(all_message_info, dim=0)\n",
    "    \n",
    "    # Convert to numpy for DataFrame creation\n",
    "    message_info = message_info.numpy()\n",
    "    \n",
    "    # Create column names\n",
    "    node_info = ['x', 'y', 'delta_x', 'delta_y', 'q', 'm'] #for 2d\n",
    "    source_cols = [f'{f}1' for f in node_info]\n",
    "    target_cols = [f'{f}2' for f in node_info]\n",
    "    message_cols = [f'e{i}' for i in range(messages.shape[-1])]\n",
    "    columns = source_cols + target_cols + message_cols\n",
    "    \n",
    "    # Create DataFrame\n",
    "    message_info = pd.DataFrame(message_info, columns=columns)\n",
    "    \n",
    "    # distances between particles\n",
    "    message_info['x_dist'] = message_info.x1 - message_info.x2\n",
    "    message_info['y_dist'] = message_info.y1 - message_info.y2\n",
    "    message_info['r'] = np.sqrt(message_info.x_dist**2 + message_info.y_dist**2)\n",
    "    \n",
    "    # Calculate relative velocities\n",
    "    message_info['dvx'] = message_info.delta_x1 - message_info.delta_x2\n",
    "    message_info['dvy'] = message_info.delta_y1 - message_info.delta_y2\n",
    "    message_info['v_rel'] = np.sqrt(message_info.dvx**2 + message_info.dvy**2)\n",
    "\n",
    "    message_cols = [col for col in message_info.columns if col.startswith('e')]\n",
    "    message_features = message_info[message_cols].values #shape is [batch_size * num edges, 100]\n",
    "    \n",
    "    return message_info, message_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "standard message model (100 outputs). plot the message "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_important_features(message_features):\n",
    "    \"\"\"get most important (top 2) features (standard) by measuring variance over all data\n",
    "\n",
    "    Args:\n",
    "        message_features (numpy.ndarray): just the message features. of shape [datapoints, 100]\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: most important two messages. of shape [datapoints, 2]\n",
    "    \"\"\"\n",
    "    std = message_features.std(axis = 0)\n",
    "    important_elements = np.argsort(std)[-2:]\n",
    "    return message_features[:, important_elements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_force(message_info, sim = 'r1'):\n",
    "    x_dist = message_info['x_dist']\n",
    "    y_dist = message_info['y_dist']\n",
    "    r = message_info['r']\n",
    "    m1m2 = message_info['m1'] * message_info['m2']\n",
    "    eps = 1e-6\n",
    "    if sim == 'r1':\n",
    "        f = - m1m2 / (r*r + eps)\n",
    "        f_x = f * x_dist / (r+eps)\n",
    "        f_y = f * y_dist / (r+eps)\n",
    "    return f, f_x, f_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
